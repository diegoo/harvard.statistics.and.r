

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Central Limit Theorem and t-distribution</title>
    
    <meta name="author" content="Rafael Irizarry and Michael Love">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="http://genomicsclass.github.io/book/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="http://genomicsclass.github.io/book/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="http://genomicsclass.github.io/book/assets/themes/twitter/css/kbroman.css" rel="stylesheet" type="text/css" media="all">
    <link href="http://genomicsclass.github.io/book/assets/themes/twitter/css/knitr.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
    <link href="http://genomicsclass.github.io/book/assets/themes/twitter/ico/favicon.ico" rel="icon" type="image/png">

    <!-- atom & rss feed -->
    <link href="http://genomicsclass.github.io/booknil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="http://genomicsclass.github.io/booknil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

    <!-- knitr javascript -->
    <script src="http://genomicsclass.github.io/book/assets/themes/twitter/knitr.js" type="text/javascript"></script>

    <!-- mathjax javascript -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>


  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="http://genomicsclass.github.io/book">PH525x series - Biomedical Data Science</a>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        

<div class="page-header">
  <h2>Central Limit Theorem and t-distribution </h2>
</div>

<div class="row-fluid">
  <div class="span12">
    <h2 id="central-limit-theorem-and-t-distribution">Central Limit Theorem and t-distribution</h2>

<p>Below we will discuss the Central Limit Theorem (CLT) and the t-distribution, both of which help us make important calculations related to probabilities. Both are frequently used in science to test statistical hypotheses. To use these, we have to make different assumptions from those for the CLT and the t-distribution. However, if the assumptions are true, then we are able to calculate the exact probabilities of events through the use of mathematical formula.</p>

<h4 id="central-limit-theorem">Central Limit Theorem</h4>

<p>The CLT is one of the most frequently used mathematical results in science. It tells us that when the sample size is large, the average <script type="math/tex">\bar{Y}</script> of a random sample follows a normal distribution centered at the population average <script type="math/tex">\mu_Y</script> and with standard deviation equal to the population standard deviation <script type="math/tex">\sigma_Y</script>, divided by the square root of the sample size <script type="math/tex">N</script>. We refer to the standard deviation of the distribution of a random variable as the random variable’s <em>standard error</em>.</p>

<p>Please note that if we subtract a constant from a random variable, the
mean of the new random variable shifts by that
constant. Mathematically, if <script type="math/tex">X</script> is a random variable with mean <script type="math/tex">\mu</script>
and <script type="math/tex">a</script> is a constant, the mean of <script type="math/tex">X - a</script> is <script type="math/tex">\mu-a</script>. A similarly
intuitive result holds for multiplication and the standard deviation (SD).
If <script type="math/tex">X</script> is a random
variable with mean <script type="math/tex">\mu</script> and SD <script type="math/tex">\sigma</script>, and <script type="math/tex">a</script> is a constant, then
the mean and SD of <script type="math/tex">aX</script> are <script type="math/tex">a \mu</script> and <script type="math/tex">\mid a \mid \sigma</script>
respectively. To see how intuitive this is, imagine that we subtract
10 grams from each of the mice weights. The average weight should also
drop by that much. Similarly, if we change the units from grams to
milligrams by multiplying by 1000, then the spread of the numbers
becomes larger.</p>

<p>This implies that if we take many samples of size <script type="math/tex">N</script>, then the quantity:</p>

<script type="math/tex; mode=display">\frac{\bar{Y} - \mu}{\sigma_Y/\sqrt{N}}</script>

<p>is approximated with a normal distribution centered at 0 and with standard deviation 1.</p>

<p>Now we are interested in the difference between two sample averages. Here again a mathematical result helps. If we have two random variables <script type="math/tex">X</script> and <script type="math/tex">Y</script> with means <script type="math/tex">\mu_X</script> and <script type="math/tex">\mu_Y</script> and variance <script type="math/tex">\sigma_X</script> and <script type="math/tex">\sigma_Y</script> respectively, then we have the following result: the mean of the sum <script type="math/tex">Y + X</script> is the sum of the means <script type="math/tex">\mu_Y + \mu_X</script>. Using one of the facts we mentioned earlier, this implies that the mean of <script type="math/tex">Y - X = Y + aX</script> with <script type="math/tex">a = -1</script> , which implies that the mean of <script type="math/tex">Y - X</script> is <script type="math/tex">\mu_Y - \mu_X</script>. This is intuitive. However, the next result is perhaps not as intuitive.  If <script type="math/tex">X</script> and <script type="math/tex">Y</script> are independent of each other, as they are in our mouse example, then the variance (SD squared) of <script type="math/tex">Y + X</script> is the sum of the variances <script type="math/tex">\sigma_Y^2 + \sigma_X^2</script>. This implies that variance of the difference <script type="math/tex">Y - X</script> is the variance of <script type="math/tex">Y + aX</script> with <script type="math/tex">a = -1</script> which is <script type="math/tex">\sigma^2_Y + a^2 \sigma_X^2 = \sigma^2_Y + \sigma_X^2</script>. So the variance of the difference is also the sum of the variances. If this seems like a counterintuitive result, remember that if <script type="math/tex">X</script> and <script type="math/tex">Y</script> are independent of each other, the sign does not really matter. It can be considered random: if <script type="math/tex">X</script> is normal with certain variance, for example, so is <script type="math/tex">-X</script>.  Finally, another useful result is that the sum of normal variables is again normal.</p>

<p>All this math is very helpful for the purposes of our study because we have two sample averages and are interested in the difference. Because both are normal, the difference is normal as well, and the variance (the standard deviation squared) is the sum of the two variances.
Under the null hypothesis that there is no difference between the population averages, the difference between the sample averages <script type="math/tex">\bar{Y}-\bar{X}</script>, with <script type="math/tex">\bar{X}</script> and <script type="math/tex">\bar{Y}</script> the sample average for the two diets respectively, is approximated by a normal distribution centered at 0 (there is no difference) and with standard deviation <script type="math/tex">\sqrt{\sigma_X^2 +\sigma_Y^2}/\sqrt{N}</script>.</p>

<p>This suggests that this ratio:</p>

<script type="math/tex; mode=display">\frac{\bar{Y}-\bar{X}}{\sqrt{\frac{\sigma_X^2}{M} + \frac{\sigma_Y^2}{N}}}</script>

<p>is approximated by a normal distribution centered at 0 and standard deviation 1.  Using this approximation makes computing p-values simple because we know the proportion of the distribution under any value. For example, only 5% of these values are larger than 2 (in absolute value):</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">pnorm</span><span class="p">(</span><span class="m">-2</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="m">2</span><span class="p">))</span><span class="w">
</span></code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>## [1] 0.04550026
</code></pre>
</div>

<p>We don’t need to buy more mice, 12 and 12 suffice.</p>

<p>However, we can’t claim victory just yet because we don’t know the population standard deviations: <script type="math/tex">\sigma_X</script> and <script type="math/tex">\sigma_Y</script>. These are unknown population parameters, but we can get around this by using the sample standard deviations, call them <script type="math/tex">s_X</script> and <script type="math/tex">s_Y</script>. These are defined as:</p>

<script type="math/tex; mode=display">s_X^2 = \frac{1}{M-1} \sum_{i=1}^M (X_i - \bar{X})^2  \mbox{ and }  s_Y^2 = \frac{1}{N-1} \sum_{i=1}^N (Y_i - \bar{Y})^2</script>

<p>Note that we are dividing by <script type="math/tex">M-1</script> and <script type="math/tex">N-1</script>, instead of by <script type="math/tex">M</script> and <script type="math/tex">N</script>. There is a theoretical reason for doing this which we do not explain here. But to get an intuition, think of the case when you just have 2 numbers. The average distance to the mean is basically 1/2 the difference between the two numbers. So you really just have information from one number. This is somewhat of a minor point. The main point is that <script type="math/tex">s_X</script> and <script type="math/tex">s_Y</script> serve as estimates of <script type="math/tex">\sigma_X</script> and <script type="math/tex">\sigma_Y</script></p>

<p>So we can redefine our ratio as</p>

<script type="math/tex; mode=display">\sqrt{N} \frac{\bar{Y}-\bar{X}}{\sqrt{s_X^2 +s_Y^2}}</script>

<p>if <script type="math/tex">M=N</script> or in general,</p>

<script type="math/tex; mode=display">\frac{\bar{Y}-\bar{X}}{\sqrt{\frac{s_X^2}{M} + \frac{s_Y^2}{N}}}</script>

<p>The CLT tells us that when <script type="math/tex">M</script> and <script type="math/tex">N</script> are large, this random variable is normally distributed with mean 0 and SD 1. Thus we can compute p-values using the function <code class="highlighter-rouge">pnorm</code>.</p>

<h4 id="the-t-distribution">The t-distribution</h4>

<p>The CLT relies on large samples, what we refer to as <em>asymptotic results</em>. When the CLT does not apply, there is another option that does not rely on asymptotic results. When the original population from which a random variable, say <script type="math/tex">Y</script>, is sampled is normally distributed with mean 0, then we can calculate the distribution of:</p>

<script type="math/tex; mode=display">\sqrt{N} \frac{\bar{Y}}{s_Y}</script>

<p>This is the ratio of two random variables so it is not
necessarily normal. The fact that the denominator can be small by
chance increases the probability of observing large
values. <a href="http://en.wikipedia.org/wiki/William_Sealy_Gosset">William Sealy Gosset</a>,
an employee of the Guinness brewing company, deciphered the
distribution of this random variable and published a paper under the
pseudonym “Student”. The distribution is therefore called Student’s
t-distribution. Later we will learn more about how this result is
used.</p>

<p>Here we will use the mice phenotype data as an example. We start by
creating two vectors, one for the control population and one for the
high-fat diet population:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">dat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"mice_pheno.csv"</span><span class="p">)</span><span class="w"> </span><span class="c1">#We downloaded this file in a previous section
</span><span class="n">controlPopulation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">filter</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="n">Sex</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"F"</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Diet</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"chow"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">  
  </span><span class="n">select</span><span class="p">(</span><span class="n">Bodyweight</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">unlist</span><span class="w">
</span><span class="n">hfPopulation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">filter</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="n">Sex</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"F"</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Diet</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"hf"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">  
  </span><span class="n">select</span><span class="p">(</span><span class="n">Bodyweight</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">unlist</span><span class="w">
</span></code></pre>
</div>

<p>It is important to keep in mind that what we are assuming to be normal here is the distribution of <script type="math/tex">y_1,y_2,\dots,y_n</script>, not the random variable <script type="math/tex">\bar{Y}</script>. Although we can’t do this in practice, in this illustrative example, we get to see this distribution for both controls and high fat diet mice:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">rafalib</span><span class="p">)</span><span class="w">
</span><span class="n">mypar</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">hfPopulation</span><span class="p">)</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">controlPopulation</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p><img src="figure/clt_and_t-distribution-population_histograms-1.png" alt="Histograms of all weights for both populations." /></p>

<p>We can use <em>qq-plots</em> to confirm that the distributions are relatively
close to being normally distributed. We will explore these plots in
more depth in a later section, but the important thing to know is that
it compares data (on the y-axis) against a theoretical distribution
(on the x-axis). If the points fall on the identity line, then the
data is close to the theoretical distribution.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">mypar</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">qqnorm</span><span class="p">(</span><span class="n">hfPopulation</span><span class="p">)</span><span class="w">
</span><span class="n">qqline</span><span class="p">(</span><span class="n">hfPopulation</span><span class="p">)</span><span class="w">
</span><span class="n">qqnorm</span><span class="p">(</span><span class="n">controlPopulation</span><span class="p">)</span><span class="w">
</span><span class="n">qqline</span><span class="p">(</span><span class="n">controlPopulation</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p><img src="figure/clt_and_t-distribution-population_qqplots-1.png" alt="Quantile-quantile plots of all weights for both populations." /></p>

<p>The larger the sample, the more forgiving the result is to the
weakness of this approximation. In the next section, we will see that
for this particular dataset the t-distribution works well even for
sample sizes as small as 3.</p>


  </div>
</div>


      </div>
      <hr>
      <footer>
        <p><small>
  <!-- start of Karl's footer; modify this part -->

<a href="http://genomicsclass.github.io/book/">PH525x</a>, 
Rafael Irizarry and Michael Love, 
<a href="https://github.com/genomicsclass/labs/blob/master/LICENSE">MIT License</a>

  <!-- end of Karl's footer; modify this part -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>

